"Learning Visuomotor Policies for Aerial Navigation Using Cross-Modal Representations","R. Bonatti; R. Madaan; V. Vineet; S. Scherer; A. Kapoor","Carnegie Mellon University,The Robotics Institute,Pittsburgh,PA; Microsoft Corporation,Redmond,WA; Microsoft Corporation,Redmond,WA; Carnegie Mellon University,The Robotics Institute,Pittsburgh,PA; Microsoft Corporation,Redmond,WA","2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","10 Feb 2021","2020","","","1637","1644","Machines are a long way from robustly solving open-world perception-control tasks, such as first-person view (FPV) aerial navigation. While recent advances in end-to- end Machine Learning, especially Imitation Learning and Reinforcement appear promising, they are constrained by the need of large amounts of difficult-to-collect labeled real- world data. Simulated data, on the other hand, is easy to generate, but generally does not render safe behaviors in diverse real-life scenarios. In this work we propose a novel method for learning robust visuomotor policies for real-world deployment which can be trained purely with simulated data. We develop rich state representations that combine supervised and unsupervised environment data. Our approach takes a cross-modal perspective, where separate modalities correspond to the raw camera data and the system states relevant to the task, such as the relative pose of gates to the drone in the case of drone racing. We feed both data modalities into a novel factored architecture, which learns a joint lowdimensional embedding via Variational Auto Encoders. This compact representation is then fed into a control policy, which we trained using imitation learning with expert trajectories in a simulator. We analyze the rich latent spaces learned with our proposed representations, and show that the use of our cross-modal architecture significantly improves control policy performance as compared to end-to-end learning or purely unsupervised feature extractors. We also present real-world results for drone navigation through gates in different track configurations and environmental conditions. Our proposed method, which runs fully onboard, can successfully generalize the learned representations and policies across simulation and reality, significantly outperforming baseline approaches.","2153-0866","978-1-7281-6212-6","10.1109/IROS45743.2020.9341049","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9341049","","Navigation;Machine learning;Logic gates;Feature extraction;Trajectory;Task analysis;Drones","autonomous aerial vehicles;cameras;feature extraction;learning (artificial intelligence)","cross-modal representations;robustly solving open-world perception-control tasks;first-person view aerial navigation;end-to- end Machine Learning;imitation learning;real- world data;safe behaviors;real-life scenarios;robust visuomotor policies;real-world deployment;rich state representations;unsupervised environment data;cross-modal perspective;separate modalities correspond;raw camera data;system states;drone racing;data modalities;joint lowdimensional embedding;compact representation;rich latent spaces;cross-modal architecture;control policy performance;end-to-end learning;purely unsupervised feature extractors;real-world results;drone navigation;learned representations","","1.0","","39.0","","10 Feb 2021","","","IEEE","IEEE Conferences"
"SE3-Pose-Nets: Structured Deep Dynamics Models for Visuomotor Control","A. Byravan; F. Leeb; F. Meier; D. Fox","University of Washington, School of Computer Science & Engineering; University of Washington, School of Computer Science & Engineering; University of Washington, School of Computer Science & Engineering; University of Washington, School of Computer Science & Engineering","2018 IEEE International Conference on Robotics and Automation (ICRA)","13 Sep 2018","2018","","","3339","3346","In this work, we present an approach to deep visuomotor control using structured deep dynamics models. Our model, a variant of SE3-Nets, learns a low-dimensional pose embedding for visuomotor control via an encoder-decoder structure. Unlike prior work, our model is structured: given an input scene, our network explicitly learns to segment salient parts and predict their pose embedding and motion, modeled as a change in the pose due to the applied actions. We train our model using a pair of point clouds separated by an action and show that given supervision only through point-wise data associations between the frames our network is able to learn a meaningful segmentation of the scene along with consistent poses. We further show that our model can be used for closed-loop control directly in the learned low-dimensional pose space, where the actions are computed by minimizing pose error using gradient-based methods, similar to traditional model-based control. We present results on controlling a Baxter robot from raw depth data in simulation and RGBD data in the real world and compare against two baseline deep networks. We also test the robustness and generalization performance of our controller under changes in camera pose, lighting, occlusion, and motion. Our method is robust, runs in real-time, achieves good prediction of scene dynamics, and outperforms baselines on multiple control runs. Video results can be found at: https://rse-lab.cs.washington.edu/se3-structured-deep-ctrl/.","2577-087X","978-1-5386-3081-5","10.1109/ICRA.2018.8461184","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8461184","","Three-dimensional displays;Predictive models;Transforms;Computational modeling;Data models;Aerospace electronics;Training","cameras;closed loop systems;control engineering computing;gradient methods;image colour analysis;image segmentation;industrial robots;learning (artificial intelligence);minimisation;neural nets;pose estimation;robot dynamics;robot vision","SE3-pose-Nets;deep visuomotor control;SE3-Nets;encoder-decoder structure;pose embedding;point-wise data associations;closed-loop control;scene dynamics;structred deep dynamics models;pose error minimization;gradient-based methods;Baxter robot","","10.0","","25.0","","13 Sep 2018","","","IEEE","IEEE Conferences"
"Phototaxic Foraging of the Archaepaddler, a Hypothetical Deep-Sea Species","R. J. V. Bertin; W. A. v. d. Grind","Laboratoire de Physiologie de la Perception et de l'Action Collège de France/C.N.R.S. 11, place Marcelin Berthelot 75005 Paris, France bertin@cdf-1ppa.in2p3.fr; Neuroethology Group Department of Comparative Physiology Utrecht University and Helmholtz Instituut School for Autonomous Systems Research Padualaan 8 3584 CH Utrecht The Netherlands W.A.vandeGrind@bio.uu.nl","Artificial Life","19 May 2014","1998","4.0","2.0","157","181","An autonomous agent (animat, hypothetical animal), called the (archae) paddler, is simulated in sufficient detail to regard its simulated aquatic locomotion (paddling) as physically possible. The paddler is supposed to be a model of an animal that might exist, although it is perfectly possible to view it as a model of a robot that might be built. The agent is assumed to navigate in a simulated deep-sea environment, where it forages for autoluminescent prey. It uses a biologically inspired phototaxic foraging strategy, while paddling in a layer just above the bottom. The advantage of this living space is that the navigation problem—and hence our model—is essentially two-dimensional. Moreover, the deep-sea environment is physically simple (and hence easy to simulate): no significant currents, constant temperature, completely dark. A foraging performance metric is developed that circumvents the necessity to solve the traveling salesman problem. A parametric simulation study then quantifies the influence of habitat factors, such as the density of prey, and body geometry (e.g., placement, direction and directional selectivity of the eyes) on foraging success. Adequate performance proves to require a specific body geometry adapted to the habitat characteristics. In general, performance degrades gracefully for modest changes of the geometric and habitat parameters, indicating that we work in a stable region of “design space.” The parameters have to strike a compromise between, on the one hand, the ability to “fixate” an attractive target, and on the other hand, to “see” as many targets at the same time as possible. One important conclusion is that simple reflex-based navigation can be surprisingly efficient. Additionally, performance in a global task (foraging) depends strongly on local parameters such as visual direction tuning, position of the eyes and paddles, and so forth. Behavior and habitat “mold” the body, and the body geometry strongly influences performance. The resulting platform enables further testing of foraging strategies or vision and locomotion theories stemming either from biology or from robotics.","1064-5462","","10.1162/106454698568503","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6787864","hypothetical animal;two-light experiment;phototaxic foraging;visuomotor behavior;foraging performance;animats","","","","","","","","","19 May 2014","","","MIT Press","MIT Press Journals"
"Visuo-motor tracking with coordinated wrist movements under different combinations of visual and kinesthetic disturbances","L. Masia; V. Squeri; M. Casadio; P. Morasso; V. Sanguineti; G. Sandini","Robotics Brain and Cognitive Sciences, Italian Institute of Technology, Genoa, Italy; Robotics Brain and Cognitive Sciences, Italian Institute of Technology, Genoa, Italy; Department of Informatics, Systems and Telecommunications, University of Genoa, Italy; Department of Informatics, Systems and Telecommunications, University of Genoa, Italy; Robotics Brain and Cognitive Sciences, Italian Institute of Technology, Genoa, Italy; Robotics Brain and Cognitive Sciences, Italian Institute of Technology, Genoa, Italy","2009 2nd Conference on Human System Interactions","23 Jun 2009","2009","","","715","718","This study addresses a major problem in the design of HCI (human-computer interface) systems: how to avoid or reduce the long learning/adaptation process and the corresponding attentional load of the underlying hand-eye coordination task that frequently affects HCI systems. In particular, we considered a 2D tracking task with two degrees of freedom of the wrist to a visual target whose frame of reference was rotated with respect to a body-fixed frame in a time varying manner. We investigated it by means of a wrist robot coupled with a virtual reality system. The experimental protocol consisted of applying kinesthetic and visual disturbances in a unimodal or bimodal manner and observing the tracking performance. The kinesthetic disturbance was provided by passively rotating the forearm of the subjects by the third degree of freedom of the wrist robot, while the visual disturbance was provided by rotating the visual scene. The results suggest that the combination of a suitable proprioceptive feedback with the kinematic redundancy of the HCI system might be a rather general principle for improving the efficiency of HCI systems.","2158-2254","978-1-4244-3959-1","10.1109/HSI.2009.5091065","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5091065","wrist robot;visuo-proprioceptive disturbance;tracking;virtual reality","Tracking;Wrist;Robot kinematics;Human computer interaction;Motor drives;Mice;Virtual reality;Brushes;Cognitive robotics;Informatics","control engineering computing;human computer interaction;robots;virtual reality","visuomotor tracking;coordinated wrist movements;visual-kinesthetic disturbances;human-computer interface;learning-adaptation process;hand-eye coordination task;virtual reality system;kinematic redundancy","","1.0","","12.0","","23 Jun 2009","","","IEEE","IEEE Conferences"
"Visual learning by imitation with motor representations","M. Lopes; J. Santos-Victor","Inst. de Sistemas e Robotica, Inst. Superior Tecnico, Lisboa, Portugal; Inst. de Sistemas e Robotica, Inst. Superior Tecnico, Lisboa, Portugal","IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)","16 May 2005","2005","35.0","3.0","438","449","We propose a general architecture for action (mimicking) and program (gesture) level visual imitation. Action-level imitation involves two modules. The viewpoint transformation (VPT) performs a ""rotation"" to align the demonstrator's body to that of the learner. The visuo-motor map (VMM) maps this visual information to motor data. For program-level (gesture) imitation, there is an additional module that allows the system to recognize and generate its own interpretation of observed gestures to produce similar gestures/goals at a later stage. Besides the holistic approach to the problem, our approach differs from traditional work in i) the use of motor information for gesture recognition; ii) usage of context (e.g., object affordances) to focus the attention of the recognition system and reduce ambiguities, and iii) use iconic image representations for the hand, as opposed to fitting kinematic models to the video sequence. This approach is motivated by the finding of visuomotor neurons in the F5 area of the macaque brain that suggest that gesture recognition/imitation is performed in motor terms (mirror) and rely on the use of object affordances (canonical) to handle ambiguous actions. Our results show that this approach can outperform more conventional (e.g., pure visual) methods.","1941-0492","","10.1109/TSMCB.2005.846654","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1430829","Anthropomorphic robots;imitation;learning;visuomotor coordination","Robot kinematics;Cognitive robotics;Humans;Robot sensing systems;Image recognition;Mirrors;Focusing;Image representation;Video sequences;Neurons","gesture recognition;image representation;psychology;robot vision;image sequences;robot kinematics;learning (artificial intelligence);humanoid robots","visual learning;motor representation;program level visual imitation;action-level imitation;viewpoint transformation;VPT;visuo-motor map;VMM map;motor data;motor information;gesture recognition system;iconic image representation;kinematic model;video sequence;visuomotor neuron;F5 area;macaque brain;anthropomorphic robot;visuomotor coordination","Algorithms;Artificial Intelligence;Biomimetics;Bionics;Computer Simulation;Hand;Hand Strength;Humans;Image Interpretation, Computer-Assisted;Models, Biological;Motor Skills;Movement;Pattern Recognition, Automated;Robotics;Vision","72.0","1.0","32.0","","16 May 2005","","","IEEE","IEEE Journals"
"Affordance Learning for End-to-End Visuomotor Robot Control","A. Hämäläinen; K. Arndt; A. Ghadirzadeh; V. Kyrki","Aalto University,Espoo,Finland; Aalto University,Espoo,Finland; Aalto University,Espoo,Finland; Aalto University,Espoo,Finland","2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","28 Jan 2020","2019","","","1781","1788","Training end-to-end deep robot policies requires a lot of domain-, task-, and hardware-specific data, which is often costly to provide. In this work, we propose to tackle this issue by employing a deep neural network with a modular architecture, consisting of separate perception, policy, and trajectory parts. Each part of the system is trained fully on synthetic data or in simulation. The data is exchanged between parts of the system as low-dimensional latent representations of affordances and trajectories. The performance is then evaluated in a zero-shot transfer scenario using Franka Panda robot arm. Results demonstrate that a low-dimensional representation of scene affordances extracted from an RGB image is sufficient to successfully train manipulator policies. We also introduce a method for affordance dataset generation, which is easily generalizable to new tasks, objects and environments, and requires no manual pixel labeling.","2153-0866","978-1-7281-4004-9","10.1109/IROS40897.2019.8968596","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8968596","","","control engineering computing;learning (artificial intelligence);manipulators;neural nets","affordance learning;end-to-end visuomotor robot control;training end-to-end deep robot policies;hardware-specific data;deep neural network;trajectory parts;synthetic data;low-dimensional latent representations;zero-shot transfer scenario;Franka Panda robot arm;low-dimensional representation;scene affordances;manipulator policies;affordance dataset generation","","3.0","","33.0","","28 Jan 2020","","","IEEE","IEEE Conferences"
"Force field training to facilitate learning visual distortions: a ""sensory crossover"" experiment","Yejun Wei; J. L. Patton","Rehabilitation Inst., Chicago, IL, USA; Rehabilitation Inst., Chicago, IL, USA","12th International Symposium on Haptic Interfaces for Virtual Environment and Teleoperator Systems, 2004. HAPTICS '04. Proceedings.","19 Apr 2004","2004","","","194","199","Previous studies on reaching movements have shown that people can adapt to distortions that are either visuomotor (e.g., prism glasses) or mechanical (e.g., force fields) through repetitive training. Other work has shown that these two types of adaptation may share similar neural resources. One effective test of this sharing hypothesis would be to show that one could teach one using the other. This study investigated whether training with a specialized force field could benefit the learning of a visual distortion. Two groups of subjects volunteered to participate in this study. One group of subjects trained directly on a visual rotation. The other group of subjects trained in a ""mixed field"" condition. The mixed field was primarily a force field that was specially designed so that, after adapting to its characteristics, the subject would make the appropriate movement in the visual rotation condition. The mixed field condition also contained intermittent test movements that evaluated performance in the visual rotation condition. Results showed that errors reduced more rapidly in the mixed field condition. We also found that subjects were able to generalize what they learned to movement directions that were not part of the training, but there was no detectable difference between the two groups. Finally, we found no difference in the rate these training effects washed out after subjects returned to normal conditions. This study shows that training with robotic forces can facilitate the learning of visual rotations. The learning may be enhanced in the mixed condition by the addition of cutaneous and proprioceptive force sensors. Moreover, this study can be applied to telerobotics and the rehabilitation of brain injured individuals, where there is often a distortion in hand-eye coordination.","","0-7695-2112-6","10.1109/HAPTIC.2004.1287196","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1287196","","Force sensors;Nonlinear distortion;Robot kinematics;Kinetic theory;Testing;Robot sensing systems;Rehabilitation robotics;Glass;Telerobotics;Brain injuries","learning (artificial intelligence);image processing;force sensors;distortion;telerobotics;neural nets","force field training;visual distortions;sensory crossover;visuomotor;prism glasses;neural resources;visual rotation;telerobotics;hand-eye coordination;robotic forces;force sensors","","2.0","","","","19 Apr 2004","","","IEEE","IEEE Conferences"
"Interpolation and Extrapolation in Human Behavior and Neural Networks","E. Guigon","INSERM U483, Université Pierre et Marie Curie","Journal of Cognitive Neuroscience","19 May 2014","2004","16.0","3.0","382","389","Unlike most artificial systems, the brain is able to face situations that it has not learned or even encountered before. This ability is not in general echoed by the properties of most neural networks. Here, we show that neural computation based on least-square error learning between populations of intensitycoded neurons can explain interpolation and extrapolation capacities of the nervous system in sensorimotor and cognitive tasks. We present simulations for function learning experiments, auditory-visual behavior, and visuomotor transformations. The results suggest that induction in human behavior, be it sensorimotor or cognitive, could arise from a common neural associative mechanism.","0898-929X","","10.1162/089892904322926728","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6790194","","","","","","","","","","19 May 2014","","","MIT Press","MIT Press Journals"
"Self-Supervised Correspondence in Visuomotor Policy Learning","P. Florence; L. Manuelli; R. Tedrake","Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology, Cambridge, MA, USA; Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology, Cambridge, MA, USA; Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology, Cambridge, MA, USA","IEEE Robotics and Automation Letters","14 Jan 2020","2020","5.0","2.0","492","499","In this letter, we explore using self-supervised correspondence for improving the generalization performance and sample efficiency of visuomotor policy learning. Prior work has primarily used approaches such as autoencoding, pose-based losses, and end-to-end policy optimization in order to train the visual portion of visuomotor policies. We instead propose an approach using self-supervised dense visual correspondence training and show that this enables visuomotor policy learning with surprisingly high generalization performance with modest amounts of data. Using imitation learning, we demonstrate extensive hardware validation on challenging manipulation tasks with as few as 50 demonstrations. Our learned policies can generalize across classes of objects, react to deformable object configurations, and manipulate textureless symmetrical objects in a variety of backgrounds, all with closed-loop, real-time vision-based policies. Simulated imitation learning experiments suggest that correspondence training offers sample complexity and generalization benefits compared to autoencoding and end-to-end training.","2377-3766","","10.1109/LRA.2019.2956365","National Science Foundation; Lockheed Martin Corporation; Amazon Research Award Grant; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8917583","Deep learning in robotics and automation;perception for grasping and manipulation;visual learning","Visualization;Training;Task analysis;Robots;Hardware;Complexity theory;Feature extraction","closed loop systems;computer vision;generalisation (artificial intelligence);learning (artificial intelligence);optimisation","autoencoding;closed-loop real-time vision-based policies;end-to-end training;sample complexity;simulated imitation learning experiments;learned policies;generalization performance;self-supervised dense visual correspondence training;end-to-end policy optimization;sample efficiency;visuomotor policy learning","","2.0","","41.0","IEEE","28 Nov 2019","","","IEEE","IEEE Journals"
"The temporal limits of agency for reaching movements in augmented virtuality","G. Bernal; P. Maes; O. A. Kannape","MIT Media Lab, Massachusetts Institute of Technology, Cambridge, USA; MIT Media Lab, Massachusetts Institute of Technology, Cambridge, USA; School of Psychology, University of Central Lancashire, Preston, UK","2016 IEEE International Conference on Systems, Man, and Cybernetics (SMC)","9 Feb 2017","2016","","","2896","2899","The sense of agency (SoA) describes the feeling of being the author and in control of one's movements. It is closely linked to automated aspects of sensorimotor control and understood to depend on one's ability to monitor the details of one's movements. As such SoA has been argued to be a critical component of self-awareness in general and contribute to presence in virtual reality environments in particular. A common approach to investigating SoA is to ask participants to perform goal-directed movements and introducing spatial or temporal visuomotor mismatches in the feedback. Feedback movements are traditionally either switched with someone else's movements using a 2D video-feed or modified by providing abstracted feedback about one's actions on a computer screen. The aim of the current study was to quantify conscious monitoring and the SoA for ecologically valid, three dimensional feedback of the participants' actual limb and movements. This was achieved by displaying an Infra-Red (IR) feed of the participants' upper limbs in an augmented virtuality environment (AVE) using a head-mounted display (HMD). Movements could be fed back in real-time (46ms system delay) or with an experimental delay of up to 570ms. As hypothesized, participant's SoA decreased with increasing temporal visuomotor mismatches (p<;.001), replicating previous findings and extending them to AVEs. In-line with this literature, we report temporal limits of 222±60ms (50% psychometric threshold) in N=28 participants. Our results demonstrate the validity of the experimental platform by replicating studies in SoA both qualitatively and quantitatively. We discuss our findings in relation to the use of virtual and mixed reality in research and implications for neurorehabilitation therapies.","","978-1-5090-1897-0","10.1109/SMC.2016.7844679","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7844679","augmented virtuality;sense of agency;presence;sensory integration;movement feedback;motion capture","Delays;Visualization;Feeds;Real-time systems;Training;Conferences;Cybernetics","augmented reality;helmet mounted displays;human computer interaction","temporal agency limits;SoA;sense of agency;infrared feed;IR feed;augmented virtuality environment;head-mounted display;temporal visuomotor mismatches;mixed reality;HMD;AVE","","1.0","","19.0","","9 Feb 2017","","","IEEE","IEEE Conferences"
"End-to-End Learning of Driving Models from Large-Scale Video Datasets","H. Xu; Y. Gao; F. Yu; T. Darrell","Univ. of California, Berkeley, Berkeley, CA, USA; Univ. of California, Berkeley, Berkeley, CA, USA; Univ. of California, Berkeley, Berkeley, CA, USA; Univ. of California, Berkeley, Berkeley, CA, USA","2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","9 Nov 2017","2017","","","3530","3538","Robust perception-action models should be learned from training data with diverse visual appearances and realistic behaviors, yet current approaches to deep visuomotor policy learning have been generally limited to in-situ models learned from a single vehicle or simulation environment. We advocate learning a generic vehicle motion model from large scale crowd-sourced video data, and develop an end-to-end trainable architecture for learning to predict a distribution over future vehicle egomotion from instantaneous monocular camera observations and previous vehicle state. Our model incorporates a novel FCN-LSTM architecture, which can be learned from large-scale crowd-sourced vehicle action data, and leverages available scene segmentation side tasks to improve performance under a privileged learning paradigm. We provide a novel large-scale dataset of crowd-sourced driving behavior suitable for training our model, and report results predicting the driver action on held out sequences across diverse conditions.","1063-6919","978-1-5386-0457-1","10.1109/CVPR.2017.376","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8099859","","Predictive models;Visualization;Data models;Computer architecture;Motion segmentation;Training","cameras;image motion analysis;image segmentation;image sequences;learning (artificial intelligence);mobile robots;road vehicles;robot vision;video signal processing","large-scale video datasets;robust perception-action models;training data;diverse visual appearances;realistic behaviors;deep visuomotor policy learning;single vehicle;generic vehicle motion model;scale crowd-sourced video data;end-to-end trainable architecture;future vehicle egomotion;instantaneous monocular camera observations;previous vehicle state;novel FCN-LSTM architecture;large-scale crowd-sourced vehicle action data;privileged learning paradigm;driver action;scene segmentation side tasks","","157.0","","27.0","","9 Nov 2017","","","IEEE","IEEE Conferences"
"A neuro-controller for robotic manipulators based on biologically-inspired visuo-motor co-ordination neural models","G. Asuni; F. Leoni; E. Guglielmelli; A. Starita; P. Dario","ARTS Lab., Pisa Univ., Italy; ARTS Lab., Pisa Univ., Italy; ARTS Lab., Pisa Univ., Italy; NA; NA","First International IEEE EMBS Conference on Neural Engineering, 2003. Conference Proceedings.","13 May 2003","2003","","","450","453","This paper presents a novel scheme for sensor-based control of robotics manipulators by means of artificial neural networks. The system is able to control simple reaching tasks by only fusing visual and proprioceptive sensory data, without computational kinematic modeling of the arm structure, Thanks to the generalization features typical of the neural approach, the same neurocontroller has been easily adapted and successfully validated for controlling different manipulators with different mechanical structures, i.e. number of degrees of freedom, link length and weight, etc. The proposed scheme is directly inspired to research results in the field of neuroscience, specifically on nervous structures and physiological mechanisms involved in sensory motor coordination. From a psychological point of view J. Piaget (1976) explained visuo-motor associations in his scheme of circular reaction. He observed how, by making endogenous movements and correlating the resulting arm and hand spatial locations, the brain allows an auto-association to be created between visual and proprioceptive sensing. The work presented in this paper is derived from the more recent DIRECT model proposed D. Bullock et al. (1993). Significant and original modifications of such model have been introduced by the authors to increase, at the same time, both system performance and the biological coherence. The proposed neurocontroller has been first simulated both in the 2-dimensional and the 3-dimensional case, and then implemented for experimental trials on two real robotic manipulators.","","0-7803-7579-3","10.1109/CNE.2003.1196858","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1196858","","Manipulators;Biological system modeling;Robot kinematics;Robot sensing systems;Neurocontrollers;Robot control;Artificial neural networks;Biology computing;Computational modeling;Neuroscience","manipulators;robot vision;neurocontrollers;sensor fusion;neural nets","neurocontroller;robotic manipulators;biologically-inspired visuomotor coordination neural models;sensor-based control;artificial neural networks;proprioceptive sensory data;sensory motor coordination;endogenous movements;biological coherence","","4.0","","11.0","","13 May 2003","","","IEEE","IEEE Conferences"
"Measurement of BOLD Changes Due to Cued Eye-Closure and Stopping During a Continuous Visuomotor Task via Model-Based and Model-Free Approaches","G. R. Poudel; R. D. Jones; C. R. H. Innes; R. Watts; P. R. Davidson; P. J. Bones","Department of Medical Physics and Bioengineering, Christchurch Hospital, New Zealand; Department of Medical Physics and Bioengineering, Christchurch Hospital, Christchurch, New Zealand; Department of Medical Physics and Bioengineering, Christchurch Hospital, Christchurch, New Zealand; Department of Physics and Astronomy, University of Canterbury, Christchurch, New Zealand; SLISystems Inc., Christchurch; Department of Electrical and Computer Engineering, University of Canterbury, Christchurch, New Zealand","IEEE Transactions on Neural Systems and Rehabilitation Engineering","7 Oct 2010","2010","18.0","5.0","479","488","As a precursor for investigation of changes in neural activity underlying lapses of responsiveness, we set up a system to simultaneously record functional magnetic resonance imaging (fMRI), eye-video, EOG, and continuous visuomotor response inside an MRI scanner. The BOLD fMRI signal was acquired during a novel 2-D tracking task in which participants (10 males, 10 females) were cued to either briefly stop tracking and close their eyes (Stop+Close) or to briefly stop tracking (Stop) only. The onset and duration of eye-closure and stopping were identified post hoc from eye-video, EOG, and visuomotor response. fMRI data were analyzed using a general linear model (GLM) and tensorial independent component analysis (TICA). The GLM-based analysis identified predominantly increased blood oxygenation level dependent (BOLD) activity during eye-closure and stopping in multisensory areas, sensory-motor integration areas, and default-mode regions. Stopping during tracking elicited increased activity in visual processing areas, sensory-motor integration areas, and premotor areas. TICA separated the spatio-temporal pattern of activity into multiple task-related networks including the 1) occipito-medial frontal eye-movement network, 2) sensory areas, 3) left-lateralized visuomotor network, and 4) fronto-parietal visuomotor network, which were modulated differently by Stop+Close and Stop. The results demonstrate the merits of using simultaneous fMRI, behavioral, and physiological recordings to investigate the mechanisms underlying complex human behaviors in the human brain. Furthermore, knowledge of widespread modulations in brain activity due to voluntary eye-closure or stopping during a continuous visuomotor task is important for studies of the brain mechanisms underlying involuntary behaviors, such as microsleeps and attention lapses, which are often accompanied by brief eye-closure and/or response failures.","1558-0210","","10.1109/TNSRE.2010.2050782","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5477176","Attention;eye-closure;functional magnetic resonance imaging (fMRI);simultaneous recording;tensorial independent component analysis;visuomotor tracking","Physics;Biomedical engineering;Independent component analysis;Hospitals;Electrooculography;Humans;Brain;Permission;Data analysis;Delay","biomedical MRI;blood;independent component analysis;neurophysiology;vision","BOLD change;cued eye closure;continuous visuomotor task;neural activity;functional magnetic resonance imaging;eye video;EOG;MRI scanner;2D tracking task;general linear model;tensorial independent component analysis;blood oxygenation level dependent;multisensory area;sensory motor integration area","Adult;Brain;Brain Mapping;Computer Simulation;Cues;Electrooculography;Eye Movements;Humans;Image Interpretation, Computer-Assisted;Magnetic Resonance Imaging;Middle Aged;Models, Neurological;Motion Perception;Movement;Oxygen Consumption;Young Adult","7.0","","34.0","","3 Jun 2010","","","IEEE","IEEE Journals"
"Enhancing Visuomotor Adaptation by Reducing Error Signals: Single-step (Aware) versus Multiple-step (Unaware) Exposure to Wedge Prisms","C. Michel; L. Pisella; C. Prablanc; G. Rode; Y. Rossetti","1INSERM U864, Espace et Action, Bron, France; Université Claude Bernard, Lyon 1; Institut Fédératif des Neurosciences de Lyon (IFNL), Lyon, France; 1INSERM U864, Espace et Action, Bron, France; Université Claude Bernard, Lyon 1; Institut Fédératif des Neurosciences de Lyon (IFNL), Lyon, France; 1INSERM U864, Espace et Action, Bron, France; Université Claude Bernard, Lyon 1; Institut Fédératif des Neurosciences de Lyon (IFNL), Lyon, France; 1INSERM U864, Espace et Action, Bron, France; Université Claude Bernard, Lyon 1; Institut Fédératif des Neurosciences de Lyon (IFNL), Lyon, France; 1INSERM U864, Espace et Action, Bron, France; Université Claude Bernard, Lyon 1; Institut Fédératif des Neurosciences de Lyon (IFNL), Lyon, France","Journal of Cognitive Neuroscience","19 May 2014","2007","19.0","2.0","341","350","Neglect patients exhibit both a lack of awareness for the spatial distortions imposed during visuomanual prism adaptation procedures, and exaggerated postadaptation negative after-effects. To better understand this unexpected adaptive capacity in brain-lesioned patients, we investigated the contribution of awareness for the optical shift to the development of prism adaptation. The lack of awareness found in neglect was simulated in a multiple-step group where healthy subjects remained unaware of the optical deviation because of its progressive stepwise increase from 2° to 10°. We contrasted this method with the classical single-step group in which subjects were aware of the visual shift because they were directly exposed to the full 10° shift. Because the number of pointing trials was identical in the two groups, the total amount of deviation exposure was 50% larger in the single-step group. Negative after-effects were examined with an open-loop pointing task performed with the adapted hand, and generalization was tested with open-loop pointing with the nonexposed hand to visual and auditory targets. The robustness of adaptation was assessed by an open-loop pointing task after a simple de-adaptation procedure. The progressive, unaware condition was associated with larger negative after-effects, transfer to the non-exposed hand for the visual and auditory pointing tasks, and greater robustness. The amount of adaptation obtained remained, nevertheless, lower than the exaggerated adaptive capacity seen in patients with neglect. Implications for the functional mechanisms and the anatomical substrates of prism adaptation are discussed.","0898-929X","","10.1162/jocn.2007.19.2.341","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6794480","","","","","","","","","","19 May 2014","","","MIT Press","MIT Press Journals"
